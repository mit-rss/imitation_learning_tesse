{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_pilotnet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkQAeiyn0Eh4"
      },
      "source": [
        "# if running on google colab, uncomment and run these lines if you want to mount your google drive\n",
        "# so you can access your drive files from within the notebook, and save written files to drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23ic0WI1jE4-"
      },
      "source": [
        "!python --version\n",
        "\n",
        "import tensorflow\n",
        "print(\"tensorflow version: \" + tensorflow.__version__)\n",
        "import tensorflow.keras\n",
        "print(\"keras version: \" + tensorflow.keras.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myREmq-PcW95"
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Lambda, Conv2D, MaxPooling2D, Dropout, Dense, Flatten, Input, Activation, add\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "import cv2 as cv\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import datetime\n",
        "import glob\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X63R9xCweIjJ"
      },
      "source": [
        "# define PilotNet architecture\n",
        "\n",
        "# pilotnet parameters for network dimension:\n",
        "IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS = 240, 200, 3       # If you decide to change your cropping you'll need to change these!\n",
        "INPUT_SHAPE = (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS)\n",
        "\n",
        "# magnitude of your estimated steering angle correction for the left/right cameras\n",
        "OFFSET_STEERING_ANGLE = 0.1                                   # NOTE: TUNE THIS and use two values if the left/right camera angles are not symmetrical\n",
        "\n",
        "# some convenience functions for preprocessing frames:\n",
        "\n",
        "def crop(image):\n",
        "    \"\"\"\n",
        "    assumes 320x240 input (images are 320 (width) x 240 (height) from the RGB cam), \n",
        "    resizes to 200x240\n",
        "    \"\"\"\n",
        "    #      (original - target)\n",
        "    # columns: (320 - 200) /2 == 60 \n",
        "    return image[:, 60:-60] \n",
        "\n",
        "def pilotnet_crop(image):\n",
        "    \"\"\"\n",
        "    assumes 320x240 input (images are 320 (width) x 240 (height) from the RGB cam), \n",
        "    resizes to 200x66 (original Nvidia paper uses these dimensions)\n",
        "    \"\"\"\n",
        "    #      (original - target)\n",
        "    # rows:    (240 - 66) / 2 == 87\n",
        "    # columns: (320 - 200) /2 == 60 \n",
        "    return image[87:-87, 60:-60] \n",
        "\n",
        "# You might want to grab more of the image area and shrink it \n",
        "# down (instead of just cropping the center of the image out), e.g.,\n",
        "# cv.resize(image,(0,0), fx=0.4, fy=0.4, interpolation=cv.INTER_AREA )\n",
        "def shrink(image):\n",
        "    return cv.resize(image, (200,66), cv.INTER_AREA)\n",
        "\n",
        "# you can try using a larger crop:\n",
        "def pilotnet_crop_large(image):\n",
        "    \"\"\"\n",
        "    assumes 320x240 input, resizes to 280x120\n",
        "    \"\"\"\n",
        "    #       (original - target)\n",
        "    # rows:      (240 - 120) / 2 == 60\n",
        "    # columns:   (320 - 280) /2 == 35\n",
        "    return image[60:-60, 20:-20] \n",
        "\n",
        "# this is the function that will be used to preprocess your images when training the \n",
        "# model below - if you want to use a different crop edit the crop helper that's called here!\n",
        "def preprocess(image, use_full_frame=False):\n",
        "    if use_full_frame:\n",
        "        return shrink(image)\n",
        "    return crop(image)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHzBBrtNy9jP"
      },
      "source": [
        "# TODO: fill in these functions, which should take in an OpenCV image and return a new image \n",
        "# that represents rotating the input image to the left and the right respectively. \n",
        "# You may find the OpenCV functions getPerspectiveTransform and warpPerspective helpful!\n",
        "\n",
        "def rotate_car_left(img):\n",
        "    raise NotImplementedError\n",
        "\n",
        "def rotate_car_right(img):\n",
        "    raise NotImplementedError"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQwAIw-ZcW-F"
      },
      "source": [
        "# load un-augmented training data\n",
        "\n",
        "# add paths to all your training data csv files to this list:\n",
        "driving_data = [\"/content/drive/MyDrive/training_data/take1.csv\"]\n",
        "\n",
        "imgs   = []\n",
        "ngls   = []\n",
        "speeds = []\n",
        "\n",
        "for f in driving_data:\n",
        "    parent_dir = os.path.dirname(f) + os.path.sep\n",
        "    with open(f) as fh:\n",
        "        for line in fh:\n",
        "            l = line.split(',')\n",
        "\n",
        "            speed = l[2]\n",
        "            speed = float(speed)\n",
        "\n",
        "            speeds.append(speed)\n",
        "\n",
        "            img = l[0].split('/')[-1]\n",
        "            imgs.append(parent_dir + img)\n",
        "\n",
        "            ngl = l[1]\n",
        "            ngl = float(ngl)\n",
        "            ngls.append(ngl)\n",
        "\n",
        "print(\"total data\", len(imgs))\n",
        "\n",
        "# visualize the results of your fake left and right camera rotations on a training data image, \n",
        "# as well as the cropped versions:\n",
        "print(\"right cam:\")\n",
        "cv2_imshow(rotate_car_left(cv.imread(imgs[0])))\n",
        "cv2_imshow(preprocess(rotate_car_left(cv.imread(imgs[0]))))\n",
        "print(\"left cam:\")\n",
        "cv2_imshow(rotate_car_right(cv.imread(imgs[0])))\n",
        "cv2_imshow(preprocess(rotate_car_right(cv.imread(imgs[0]))))\n",
        "print(\"center cam:\")\n",
        "cv2_imshow(cv.imread(imgs[0]))\n",
        "cv2_imshow(preprocess(cv.imread(imgs[0])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAUeUsYWcW-H"
      },
      "source": [
        "# example of histograms that can help you examine what sort of steering angles and speeds your training data contains\n",
        "# (note that unless you're trying to modify the model architecture to also predict speed, your speed histogram should only show one value)\n",
        "\n",
        "fig, ax = plt.subplots(ncols=2)\n",
        "ax[0].hist(ngls, bins=50);\n",
        "ax[1].hist(speeds, bins=50);\n",
        "ax[0].set_title(\"steering angles\")\n",
        "ax[1].set_title(\"speeds\")\n",
        "fig.set_size_inches(8,5);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YiUUl1ZcW-I"
      },
      "source": [
        "# example of visualizing a random preprocessed image from your training data\n",
        "\n",
        "random_id = np.random.randint(0, len(imgs) - 1)\n",
        "\n",
        "img = preprocess(cv.imread(imgs[random_id], cv.IMREAD_COLOR))\n",
        "\n",
        "print(\"angle: {:02.3f}\".format(ngls[random_id]))\n",
        "print(\"image size:\", img.shape)\n",
        "\n",
        "rgb = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
        "fig, ax = plt.subplots()\n",
        "ax.imshow(rgb)\n",
        "fig.set_size_inches(15,5);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PL8iLwEecW-J"
      },
      "source": [
        "'''\n",
        "IMAGE AUGMENTATIONS\n",
        "\n",
        "Here are some examples of image augmentation. One simply increases the brightness (\"value\" channel in the HSV colorspace), \n",
        "another perturbs the gamma value of the input image. Try other augmentation techniques to increase the model's robustness. \n",
        "\n",
        "Note that it can take some experimentation to determine which augmentations help - some augmentation strategies might \n",
        "hurt performance. For instance, would it make sense to flip images horizontally or vertically?\n",
        "'''\n",
        "\n",
        "def increase_brightness(img):\n",
        "    # perceptually a bit more uniform than perturb_gamma\n",
        "    value = np.random.randint(20,60)\n",
        "    hsv = cv.cvtColor(img, cv.COLOR_BGR2HSV)\n",
        "    h, s, v = cv.split(hsv)\n",
        "\n",
        "    lim = 255 - value\n",
        "    v[v > lim] = 255\n",
        "    v[v <= lim] += value\n",
        "\n",
        "    final_hsv = cv.merge((h, s, v))\n",
        "    img = cv.cvtColor(final_hsv, cv.COLOR_HSV2BGR)\n",
        "    return img\n",
        "\n",
        "def make_random_gamma():\n",
        "    random_gamma = np.random.normal(1, 0.3)\n",
        "    random_gamma = np.clip(random_gamma, 0.5, 2)\n",
        "    return random_gamma\n",
        "\n",
        "def perturb_gamma(img): # https://stackoverflow.com/a/51174313\n",
        "    gamma = make_random_gamma()\n",
        "    invGamma = 1.0 / gamma\n",
        "    table = np.array([\n",
        "        ((i / 255.0) ** invGamma) * 255\n",
        "        for i in np.arange(0, 256)])\n",
        "    return cv.LUT(img, table.astype(np.uint8))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yh_zKXN-z4Ox"
      },
      "source": [
        "# augment the training data - can take a while, so only rerun this cell if you don't have augmented data already saved in a csv\n",
        "\n",
        "new_imgs = []\n",
        "angles = []\n",
        "\n",
        "for i in range(len(imgs)):\n",
        "    print(\"augmenting img {} out of {}\".format(i + 1, len(imgs)))\n",
        "\n",
        "    img = imgs[i]\n",
        "    angle = ngls[i]\n",
        "\n",
        "    new_imgs.append(img)\n",
        "    angles.append(angle)\n",
        "\n",
        "    bgr = cv.imread(img, cv.IMREAD_COLOR)\n",
        "\n",
        "    bright = increase_brightness(bgr)\n",
        "    name = img.replace(\".jpg\", \"_bright.jpg\")\n",
        "    cv.imwrite(name, bright)\n",
        "    new_imgs.append(name)\n",
        "    angles.append(angle)\n",
        "\n",
        "    gamma = perturb_gamma(bgr)\n",
        "    name = img.replace(\".jpg\", \"_gamma.jpg\")\n",
        "    cv.imwrite(name, gamma)\n",
        "    new_imgs.append(name)\n",
        "    angles.append(angle)\n",
        "\n",
        "    rotate_left = rotate_car_left(bgr)\n",
        "    name = img.replace(\".jpg\", \"_rightcam.jpg\")\n",
        "    cv.imwrite(name, rotate_left)\n",
        "    new_imgs.append(name)\n",
        "    angles.append(angle - OFFSET_STEERING_ANGLE)\n",
        "\n",
        "    rotate_right = rotate_car_right(bgr)\n",
        "    name = img.replace(\".jpg\", \"_leftcam.jpg\")\n",
        "    cv.imwrite(name, rotate_right)\n",
        "    new_imgs.append(name)\n",
        "    angles.append(angle + OFFSET_STEERING_ANGLE)\n",
        "\n",
        "    # TODO: consider also augmenting left and right images with increased brightness, gamma perturbation, etc\n",
        "    # (right now only the center image is given these augmentations)\n",
        "\n",
        "imgs = new_imgs\n",
        "ngls = angles"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHZqkEvSv3bV"
      },
      "source": [
        "# save augmented data for reloading later - make sure you delete the old one first if you're trying to save new augmented data\n",
        "with open(\"/content/drive/MyDrive/training_data/augmented_data.csv\", \"a\") as f:\n",
        "    for i in range(len(imgs)):\n",
        "        f.write(\"{},{}\\n\".format(imgs[i], ngls[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOgMQoELwvHq"
      },
      "source": [
        "# reload augmented data from csv - fill in correct filename below\n",
        "\n",
        "imgs = []\n",
        "ngls = []\n",
        "\n",
        "f = \"/content/drive/MyDrive/training_data/augmented_data.csv\"\n",
        "parent_dir = os.path.dirname(f) + os.path.sep\n",
        "with open(f) as fh:\n",
        "    for line in fh:\n",
        "        l = line.split(',')\n",
        "\n",
        "        img = l[0].split('/')[-1]\n",
        "        imgs.append(parent_dir + img)\n",
        "\n",
        "        ngl = l[1]\n",
        "        ngl = float(ngl)\n",
        "        ngls.append(ngl)\n",
        "\n",
        "print(len(imgs), len(ngls))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2wa8h_NcW-Q"
      },
      "source": [
        "# split data into training and validation subsets\n",
        "# optionally randomize order and/or use a fixed seed\n",
        "# you can also play with the fraction of the data reserved for validation\n",
        "\n",
        "VAL_SIZE_FRACTION = 0.10\n",
        "SEED = 56709\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    imgs, \n",
        "    ngls, \n",
        "    test_size=VAL_SIZE_FRACTION, \n",
        "    shuffle=True #False - TODO\n",
        ")\n",
        "#,random_state=SEED)\n",
        "\n",
        "print(len(X_train), len(X_valid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwU6od-LcW-R"
      },
      "source": [
        "# feel free to adjust the dropout rate, as well as play with the architecture of the model\n",
        "\n",
        "def build_model(dropout_rate=0.5):\n",
        "    model = Sequential()\n",
        "    model.add(Lambda(lambda x: x/127.5-1.0, input_shape=INPUT_SHAPE)) # normalize the data\n",
        "    model.add(Conv2D(24, (5,5), strides=(2, 2), activation='elu'))\n",
        "    model.add(Conv2D(36, (5,5), strides=(2, 2), activation='elu'))\n",
        "    model.add(Conv2D(48, (5,5), strides=(2, 2), activation='elu'))\n",
        "    model.add(Conv2D(64, (3,3), activation='elu'))\n",
        "    model.add(Conv2D(64, (3,3), activation='elu'))\n",
        "    model.add(Dropout(dropout_rate)) \n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(100, activation='elu'))\n",
        "    model.add(Dense(50, activation='elu'))\n",
        "    model.add(Dense(10, activation='elu'))\n",
        "    model.add(Dense(1))\n",
        "    model.summary() # prints out the model description\n",
        "    return model\n",
        "\n",
        "model = build_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xs6dMzsacW-S"
      },
      "source": [
        "# explore different learning rates and feel free to adjust the loss and optimizer as well!\n",
        "\n",
        "model.compile(loss='mean_squared_error', optimizer=Adam(lr=1.0e-4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6I8YbNPcW-U"
      },
      "source": [
        "# right now training and validation batches are treated equally - feel free to use the is_training argument to \n",
        "# implement different behavior for training and validation\n",
        "\n",
        "def batch_generator(image_paths, steering_angles, batch_size, is_training):\n",
        "    \"\"\"\n",
        "    Generate training image given image paths and associated steering angles\n",
        "    \"\"\"\n",
        "    images = np.empty([batch_size, IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS])\n",
        "    steers = np.empty(batch_size)\n",
        "    while True:\n",
        "        i = 0\n",
        "        for index in np.random.permutation(len(image_paths)):\n",
        "            \n",
        "            image = cv.imread(image_paths[index])\n",
        "\n",
        "            image = preprocess(image)\n",
        "\n",
        "            groundtruth_steering_angle = steering_angles[index]\n",
        "            \n",
        "            images[i] = image\n",
        "            steers[i] = groundtruth_steering_angle\n",
        "            \n",
        "            i += 1\n",
        "            if i == batch_size:\n",
        "                break\n",
        "        yield images, steers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgHfdTMxcW-W"
      },
      "source": [
        "# these histograms of steering angles and speeds can be useful for sanity checking and considering what you should set `OFFSET_STEERING_ANGLE` to\n",
        "\n",
        "fig, ax = plt.subplots(ncols=2)\n",
        "ax[0].hist(y_train, bins=50);\n",
        "ax[0].set_title(\"training steering angles\")\n",
        "ax[1].hist(y_valid, bins=50);\n",
        "ax[1].set_title(\"validation steering angles\")\n",
        "fig.set_size_inches(8,5);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8Xw4iWRcW-X"
      },
      "source": [
        "Note that comparing training and validation loss is not a perfect measurement of how your model will perform. Solely using mean squared error on steering angles is perhaps a crude accuracy metric on this task. Also note that if your validation data set is not characteristic of your training data set (see histograms), then the training and validation losses may be tough to compare."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kaVG-53cW-Y",
        "scrolled": true
      },
      "source": [
        "# explore different batch sizes, steps per epoch, and epochs\n",
        "\n",
        "BATCH_SIZE=20\n",
        "model.fit_generator(generator=batch_generator(X_train, y_train, batch_size=BATCH_SIZE, is_training=True),\n",
        "                    steps_per_epoch=2000,\n",
        "                    epochs=2,\n",
        "                    validation_data=batch_generator(X_valid, y_valid, batch_size=BATCH_SIZE, is_training=False),\n",
        "                    validation_steps=len(X_valid) // BATCH_SIZE,\n",
        "                    verbose=1);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuxUKnQ8QwjP"
      },
      "source": [
        "# save model for future reloading - change the name of the directory you'd like to save to\n",
        "model.save(\"/content/drive/MyDrive/training_data/wall_follower_model\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}